{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Simulation\n",
    "===\n",
    "At the very first beginning, we used OpenSim Framework to simulate body motion in the environment `L2RunEnv`, which is the environment for the machine learning problem.\n",
    "The framework outputs 41 observation variables in each step, including coordinates and velocities of points on the body, and receives 18 action values associated with 18 muscles for the next step. In this demo, we recorded all observations and actions, as well as the reward value given by the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from osim.env import L2RunEnv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 20180101\n",
    "rng = np.random.RandomState(SEED)\n",
    "\n",
    "env = L2RunEnv(visualize=False)\n",
    "# Obtain the dimension observation space and action space\n",
    "dim_obs = env.get_observation_space_size()\n",
    "dim_act = env.get_action_space_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_desc = ['joint_pos_ground_pelvis_1','joint_pos_ground_pelvis_2','joint_pos_ground_pelvis_3',\n",
    "            'joint_vel_ground_pelvis_1','joint_vel_ground_pelvis_2','joint_vel_ground_pelvis_3',\n",
    "            'joint_pos_hip_l', 'joint_vel_hip_l', 'joint_pos_hip_r', 'joint_vel_hip_r',\n",
    "            'joint_pos_knee_l', 'joint_vel_knee_l', 'joint_pos_knee_r', 'joint_vel_knee_r',\n",
    "            'joint_pos_ankle_l', 'joint_vel_ankle_l', 'joint_pos_ankle_r', 'joint_vel_ankle_r',\n",
    "            'body_pos_head_1', 'body_pos_head_2', \n",
    "            'body_pos_pelvis_1', 'body_pos_pelvis_2', \n",
    "            'body_pos_torso_1', 'body_pos_torso_2', \n",
    "            'body_pos_toes_l_1', 'body_pos_toes_l_2','body_pos_toes_r_1', 'body_pos_toes_r_2',\n",
    "            'body_pos_talus_l_1', 'body_pos_talus_l_2','body_pos_talus_r_1', 'body_pos_talus_r_2',\n",
    "            'mass_center_pos_1', 'mass_center_pos_2', 'mass_center_vel_1', 'mass_center_vel_2',\n",
    "            'misc_1', 'misc_2', 'misc_3', 'misc_4', 'misc_5']\n",
    "act_desc = ['action'+str(i) for i in range(1,dim_act+1)]\n",
    "columns_label=columns=obs_desc+act_desc+['reward']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is to obtain a policy to let the body act based on the observation. In the demo, we do not need to get a specific policy but to show how the simulation works. Henceforth, we used an initially randomized matrix representing a transformation from observations to actions:\n",
    "$$A_{i}=P_{ij}\\cdot O_{j}$$\n",
    "where $A_{i}$ is the $i$-th action variable, $P_{ij}$ is the policy matrix component, and $O_{j}$ is the $j$-th observation value. We can obtain each action under a specific set of observations by the policy.\n",
    "The framework can give the reward and job-done signal in the simulation and we recorded them for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Record the observations and actions for each step\n",
    "df = pd.DataFrame(data=[], columns=columns_label)\n",
    "# Start a new simulation\n",
    "observation = env.reset()\n",
    "# Initialize the policy matrix\n",
    "policy_mat = rng.rand(dim_act,dim_obs)\n",
    "# At the first step, set the reward to zero\n",
    "reward = 0\n",
    "# Run the simulation until the framework stop it\n",
    "done = False\n",
    "while not done:\n",
    "    # get the action based on the policy\n",
    "    action = np.dot(policy_mat, np.array(observation))\n",
    "    # record current observations and its associated actions\n",
    "    logging_row = pd.DataFrame(data=[observation+action.tolist()+[reward]], columns=columns_label)\n",
    "    df = df.append(logging_row, ignore_index=True)\n",
    "    # evolve the system to the next time stamp\n",
    "    observation, reward, done, info = env.step(action)\n",
    "df.to_csv(\"basic_sim.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_osim_rl",
   "language": "python",
   "name": "opensim-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
